{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "78498f0b-e7b4-4fbe-9e40-827fdf2025eb",
      "metadata": {
        "id": "78498f0b-e7b4-4fbe-9e40-827fdf2025eb"
      },
      "source": [
        "#### Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a24a8dd-3d4b-47fd-b884-6924e46f061a",
      "metadata": {
        "id": "9a24a8dd-3d4b-47fd-b884-6924e46f061a",
        "outputId": "24433d98-ab6b-48dc-e720-f9a3790b6542"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-02-06 14:47:06.774350: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
          ]
        }
      ],
      "source": [
        "import sys\n",
        "sys.path.append('BEHRT/Early_integration/')\n",
        "sys.path.append('BEHRT/')\n",
        "\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, Sampler\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pytorch_pretrained_bert as Bert\n",
        "import sklearn\n",
        "\n",
        "from ray.tune import CLIReporter\n",
        "from ray.tune.schedulers import ASHAScheduler\n",
        "from ray import tune\n",
        "\n",
        "from Utils import optimiser\n",
        "from Utils.common import create_folder\n",
        "import sklearn.metrics as skm\n",
        "import math\n",
        "from torch.utils.data.dataset import Dataset\n",
        "import random\n",
        "import numpy as np\n",
        "import time\n",
        "from Utils.utils import *\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from Utils.dataLoader_utils import ImbSampler, OverSampler, StratifiedSampler\n",
        "from Utils.NextXVisit_v2 import NextVisit\n",
        "from Models.BertForClassification import BertForClassification\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "#### ## ROC\n",
        "import sklearn.metrics as metrics\n",
        "import seaborn as sns\n",
        "## Plot results\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "import seaborn as sn\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "\n",
        "from Utils.add_endpoints import add_endp\n",
        "from Utils.handle_file import handle_file\n",
        "\n",
        "hf = handle_file()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd337188-4d02-4014-9ca2-7ac4c670566c",
      "metadata": {
        "id": "dd337188-4d02-4014-9ca2-7ac4c670566c"
      },
      "source": [
        "#### Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89488d8a-aee0-4ef7-8876-b078312f00be",
      "metadata": {
        "id": "89488d8a-aee0-4ef7-8876-b078312f00be"
      },
      "outputs": [],
      "source": [
        "def get_train_test_valid_idx(dataset):\n",
        "    patient_idx = dataset.index\n",
        "\n",
        "    train_idx = hf._load_pkl(file_config['train_idx'])\n",
        "    train_idx = [idx for idx in train_idx if idx in patient_idx]\n",
        "\n",
        "    test_idx = hf._load_pkl(file_config['test_idx'])\n",
        "    test_idx = [idx for idx in test_idx if idx in patient_idx]\n",
        "\n",
        "    valid_idx = hf._load_pkl(file_config['valid_idx'])\n",
        "    valid_idx = [idx for idx in valid_idx if idx in patient_idx]\n",
        "\n",
        "    return train_idx, test_idx, valid_idx\n",
        "\n",
        "def split_data(dataset):\n",
        "    train_idx, test_idx, valid_idx = get_train_test_valid_idx(dataset)\n",
        "\n",
        "\n",
        "    train = dataset.loc[train_idx ,:]\n",
        "    valid = dataset.loc[valid_idx ,:]\n",
        "    test = dataset.loc[test_idx ,:]\n",
        "\n",
        "    train = train.rename_axis('patid')\n",
        "    valid = valid.rename_axis('patid')\n",
        "    test = test.rename_axis('patid')\n",
        "\n",
        "    train.reset_index(inplace=True)\n",
        "    valid.reset_index(inplace=True)\n",
        "    test.reset_index(inplace=True)\n",
        "\n",
        "    train['patid'] = train['patid'].replace(train.patid.values,range(len(train.patid.values)))\n",
        "    valid['patid'] = valid['patid'].replace(valid.patid.values,range(len(valid.patid.values)))\n",
        "    test['patid'] = test['patid'].replace(test.patid.values,range(len(test.patid.values)))\n",
        "\n",
        "    return train, test, valid\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0e82e60-8092-4381-b351-af72bde3905b",
      "metadata": {
        "id": "c0e82e60-8092-4381-b351-af72bde3905b"
      },
      "source": [
        "#### Performing checks for the resources available"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff028d4d-d7da-48d4-81f4-33f8e04ecb04",
      "metadata": {
        "id": "ff028d4d-d7da-48d4-81f4-33f8e04ecb04"
      },
      "outputs": [],
      "source": [
        "# Check if there is a gpu available\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"There are %d GPU(s) available. \" %torch.cuda.device_count())\n",
        "    print(\"We will use the GPU: {}\".format(torch.cuda.get_device_name(0)))\n",
        "\n",
        "else:\n",
        "    print(\"No GPU available, using the CPU instead\")\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "\"\"\" Empty the cache to enable the use of the gpu \"\"\"\n",
        "\n",
        "def empty_cuda():\n",
        "    torch.cuda.empty_cache()\n",
        "    print(torch.cuda.memory_summary(device=None, abbreviated=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a691f96-f00a-4748-9f42-2cb9d22a8033",
      "metadata": {
        "id": "9a691f96-f00a-4748-9f42-2cb9d22a8033"
      },
      "source": [
        "#### Run Binary Classification\n",
        "1. Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5adc5959-e763-4719-8d36-25c616420c25",
      "metadata": {
        "id": "5adc5959-e763-4719-8d36-25c616420c25"
      },
      "outputs": [],
      "source": [
        "## Embeddings used for the classification\n",
        "\n",
        "pretrain_model_path =  ''   # pretrained MLM path\n",
        "\n",
        "file_config = { 'data':'',\n",
        "                'labels' : '',\n",
        "                'train_idx' : '',\n",
        "                'valid_idx' : '',\n",
        "                'test_idx' : '',\n",
        "                }\n",
        "\n",
        "optim_config = {\n",
        "    'lr': 5e-5,\n",
        "    'warmup_proportion': 0.1,\n",
        "    'weight_decay': 0.01\n",
        "}\n",
        "\n",
        "global_params = {\n",
        "    'batch_size': 32,\n",
        "    'gradient_accumulation_steps': 4,\n",
        "    'device': device ,\n",
        "    'output_dir': '', # output folder\n",
        "    'best_name': 'model_outp',  # output model name\n",
        "    'max_len_seq': 512, #100,\n",
        "    'max_age': 110,\n",
        "    'max_delay': 30, ## = 30years\n",
        "    'age_month': 12,\n",
        "    'delay_month': 0.25,\n",
        "    'age_year': False,\n",
        "    'age_symbol': None,\n",
        "    'min_visit': 2,\n",
        "}\n",
        "\n",
        "feature_dict = {\n",
        "            'word':True,\n",
        "            'seg':True,\n",
        "            'age':True,\n",
        "            'modalities': True,\n",
        "            'delays': True,\n",
        "            'position': True\n",
        "                }\n",
        "\n",
        "class BertConfig(Bert.modeling.BertConfig):\n",
        "    def __init__(self, config):\n",
        "        super(BertConfig, self).__init__(\n",
        "            vocab_size_or_config_json_file=config.get('vocab_size'),\n",
        "            hidden_size=config['hidden_size'],\n",
        "            num_hidden_layers=config.get('num_hidden_layers'),\n",
        "            num_attention_heads=config.get('num_attention_heads'),\n",
        "            intermediate_size=config.get('intermediate_size'),\n",
        "            hidden_act=config.get('hidden_act'),\n",
        "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
        "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
        "            max_position_embeddings = config.get('max_position_embedding'),\n",
        "            initializer_range=config.get('initializer_range'),\n",
        "        )\n",
        "        self.delays_vocab_size = config.get('delays_vocab_size')\n",
        "        self.modalities_vocab_size = config.get('modalities_vocab_size')\n",
        "        self.age_vocab_size = config.get('age_vocab_size')\n",
        "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
        "        self.output_attentions = config.get('output_attentions')\n",
        "        self.chunk_size_feed_forward = config.get('chunk_size_feed_forward')\n",
        "        self.is_decoder = config.get('is_decoder')\n",
        "        self.layer_norm_eps = config.get('layer_norm_eps')\n",
        "        self.add_cross_attention = config.get('add_cross_attention')\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "294488fe-1f32-4538-9bfc-759d10af5a9a",
      "metadata": {
        "id": "294488fe-1f32-4538-9bfc-759d10af5a9a"
      },
      "source": [
        "2. Import data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Import train data \"\"\"\n",
        "data =  hf._load_pkl(file_config['data'])\n",
        "\n",
        "\n",
        "\"\"\"Import labels \"\"\"\n",
        "labels = hf._load_csv(file_config['labels'])\n",
        "labels.rename(columns={'status_rfs_surg_5y': 'label'}, inplace=True)\n",
        "\n",
        "\"\"\"Merge label and data \"\"\"\n",
        "data = pd.merge(data, labels, on=['Num_dossier'])\n",
        "data.set_index('Num_dossier', inplace=True)\n",
        "\n",
        "# Display 3 first rows\n",
        "data.head(3)"
      ],
      "metadata": {
        "id": "52Fu8fmY7zE5"
      },
      "id": "52Fu8fmY7zE5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78566f80-bdd7-4676-8526-ea41e4f81076",
      "metadata": {
        "id": "78566f80-bdd7-4676-8526-ea41e4f81076"
      },
      "outputs": [],
      "source": [
        "# remove patients with visits less than min visit\n",
        "previous_shape = data.shape[0]\n",
        "\n",
        "data['length'] = data['inputs_quantiles_preprocessed_100_therap_removed_tolist'].apply(lambda x: len([i for i in range(len(x)) if x[i] == 'SEP']))\n",
        "data = data[data['length'] >= global_params['min_visit']]\n",
        "#data = data.reset_index(drop=True)\n",
        "\n",
        "print(\"We lost {} patient\".format(previous_shape - data.shape[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ea25fbf-a06e-48be-91fd-95154426e1f0",
      "metadata": {
        "id": "8ea25fbf-a06e-48be-91fd-95154426e1f0"
      },
      "outputs": [],
      "source": [
        "whole_seq = hf._load_pkl('') #data with the entire history\n",
        "\n",
        "tokenVocab, _ = input_vocab(inputs = data.inputs, symbol=global_params['age_symbol'])\n",
        "ageVocab, _ = age_vocab(max_age=global_params['max_age'], mon = global_params['age_month'], symbol=global_params['age_symbol'])\n",
        "modalitiesVocab, _ = mod_vocab(data.modalities_100_therap_removed, symbol=global_params['age_symbol'])\n",
        "delayVocab, _ = delay_vocab(max_delay=global_params['max_delay'], mon = global_params['delay_month'], symbol=global_params['age_symbol'])\n",
        "\n",
        "# Binary classification\n",
        "labelVocab = {0:0, 1:1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8555749-2489-4391-8d79-e49d95cead84",
      "metadata": {
        "id": "a8555749-2489-4391-8d79-e49d95cead84"
      },
      "outputs": [],
      "source": [
        "model_config = {\n",
        "    'vocab_size': len(tokenVocab), # number of disease + symbols for word embedding\n",
        "    'hidden_size': 288, # word embedding and seg embedding hidden size\n",
        "    'seg_vocab_size': 2, # number of vocab for seg embedding\n",
        "    'modalities_vocab_size': len(modalitiesVocab),\n",
        "    'age_vocab_size': len(ageVocab), # number of vocab for age embedding\n",
        "    'delays_vocab_size': len(delayVocab),\n",
        "    'max_position_embedding':global_params['max_len_seq'],  # maximum number of tokens\n",
        "    'hidden_dropout_prob': 0.3, # dropout rate\n",
        "    'num_hidden_layers': 6, # number of multi-head attention layers required\n",
        "    'num_attention_heads': 12, # number of attention heads\n",
        "    'attention_probs_dropout_prob': 0.2, # multi-head attention dropout rate\n",
        "    'intermediate_size': 512, # the size of the \"intermediate\" layer in the transformer encoder\n",
        "    'hidden_act': 'gelu', # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
        "    'initializer_range': 0.02, # parameter weight initializer range\n",
        "    'chunk_size_feed_forward' : 0,\n",
        "    'use_return_dict': True,\n",
        "    'output_attentions': True,\n",
        "    'output_hidden_states':True,\n",
        "    'is_decoder': False,\n",
        "    'layer_norm_eps' : 1e-12,  #1e-5\n",
        "    'add_cross_attention' : False\n",
        "}\n",
        "\n",
        "feature_dict = {\n",
        "    'word':True,\n",
        "    'seg':True,\n",
        "    'age':True,\n",
        "    'modalities': True,\n",
        "    'delays': True,\n",
        "    'position': True\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4740134a-9c2e-46b3-90f8-5ac1679fe422",
      "metadata": {
        "id": "4740134a-9c2e-46b3-90f8-5ac1679fe422"
      },
      "source": [
        "3. Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5b491ef3-6c1c-4e33-861e-316113f5db51",
      "metadata": {
        "id": "5b491ef3-6c1c-4e33-861e-316113f5db51"
      },
      "outputs": [],
      "source": [
        "train, test, valid = split_data(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86e9ce80-339f-429e-ac8d-78ceb6e3112e",
      "metadata": {
        "id": "86e9ce80-339f-429e-ac8d-78ceb6e3112e"
      },
      "outputs": [],
      "source": [
        "kwargs = {'num_workers': 1, 'pin_memory': True} if device=='cuda:0' else {'num_workers': 5}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32f20a3c-dd46-448b-ad4b-560d8c5fc5b2",
      "metadata": {
        "id": "32f20a3c-dd46-448b-ad4b-560d8c5fc5b2"
      },
      "outputs": [],
      "source": [
        "X_train = train['inputs_curve_intersections_preprocessed_100_therap_removed_tolist']\n",
        "y_train = train['label']\n",
        "\n",
        "Dset = FinetuneDataset(token2idx=tokenVocab, label2idx=labelVocab, mod2idx=modalitiesVocab, age2idx=ageVocab, del2idx=delayVocab, dataframe=train, max_len=global_params['max_len_seq'],  code='inputs_curve_intersections_preprocessed_100_therap_removed_tolist', delay ='delays_100_therap_removed_into_chunks_tolist'\n",
        "                              , age = \"age_100_therap_removed_into_chunks_tolist\", mod='modalities_100_therap_removed_into_chunks_tolist' )\n",
        "hp_generator = {'batch_size': global_params['batch_size'], 'balanced': 'balanced', 'shuffle':True}\n",
        "trainload = DataLoader(dataset=Dset, batch_size=global_params['batch_size'],  sampler = StratifiedSampler(X_train, y_train, batch_size=global_params['batch_size']) , **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50d60f4a-2d04-446e-bb91-70d9cb0cec11",
      "metadata": {
        "id": "50d60f4a-2d04-446e-bb91-70d9cb0cec11"
      },
      "outputs": [],
      "source": [
        "\n",
        "Dset = FinetuneDataset(token2idx=tokenVocab, label2idx=labelVocab, mod2idx=modalitiesVocab, age2idx=ageVocab, del2idx=delayVocab, dataframe=valid, max_len=global_params['max_len_seq'],  code='inputs_curve_intersections_preprocessed_100_therap_removed_tolist', delay ='delays_100_therap_removed_into_chunks_tolist'\n",
        "                              , age = \"age_100_therap_removed_into_chunks_tolist\", mod='modalities_100_therap_removed_into_chunks_tolist' )\n",
        "validload = DataLoader(dataset=Dset, batch_size=global_params['batch_size'],  shuffle = True, **kwargs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bb5045c-9b2c-4d6f-9378-267b39351135",
      "metadata": {
        "id": "5bb5045c-9b2c-4d6f-9378-267b39351135"
      },
      "outputs": [],
      "source": [
        "Dset = FinetuneDataset(token2idx=tokenVocab, label2idx=labelVocab, mod2idx=modalitiesVocab, age2idx=ageVocab, del2idx=delayVocab, dataframe=test, max_len=global_params['max_len_seq'], code='inputs_curve_intersections_preprocessed_100_therap_removed_tolist', delay ='delays_100_therap_removed_into_chunks_tolist'\n",
        "                              , age = \"age_100_therap_removed_into_chunks_tolist\", mod='modalities_100_therap_removed_into_chunks_tolist' )\n",
        "testload = DataLoader(dataset=Dset, batch_size=global_params['batch_size'],  shuffle = False, **kwargs)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd1ed32d-08af-404b-8c4b-7735a076dbfa",
      "metadata": {
        "id": "dd1ed32d-08af-404b-8c4b-7735a076dbfa"
      },
      "source": [
        "#### Model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "433d467b-5f86-4d26-b739-0ff3799ac042",
      "metadata": {
        "id": "433d467b-5f86-4d26-b739-0ff3799ac042"
      },
      "source": [
        "1. Model settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d54f911d-c46d-48db-829a-a06113bc316f",
      "metadata": {
        "tags": [],
        "id": "d54f911d-c46d-48db-829a-a06113bc316f"
      },
      "outputs": [],
      "source": [
        "def load_model(path, model):\n",
        "    # load pretrained model and update weights\n",
        "    pretrained_dict = torch.load(path)\n",
        "    model_dict = model.state_dict()\n",
        "\n",
        "    # 1. filter out unnecessary keys\n",
        "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
        "\n",
        "    # 2. overwrite entries in the existing state dict\n",
        "    model_dict.update(pretrained_dict)\n",
        "\n",
        "    # 3. load the new state dict\n",
        "    model.load_state_dict(model_dict)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e25c5654-01b7-478a-bce7-77e80da53b0f",
      "metadata": {
        "id": "e25c5654-01b7-478a-bce7-77e80da53b0f"
      },
      "outputs": [],
      "source": [
        "def define_model(model_config):\n",
        "    # del model\n",
        "\n",
        "    class_weights = torch.tensor(config['class_weights'])\n",
        "    conf = BertConfig(model_config)\n",
        "    model = BertForClassification(conf, num_labels=len(labelVocab.keys()), feature_dict=feature_dict, class_weights = class_weights)\n",
        "    model = load_model(pretrain_model_path, model)\n",
        "    model = model.to(global_params['device'])\n",
        "    optim = optimiser.adam(params=list(model.named_parameters()), config=model_config)\n",
        "\n",
        "    return model, optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82ac5f34-2937-4ea6-81e0-afdf1aeedd2c",
      "metadata": {
        "id": "82ac5f34-2937-4ea6-81e0-afdf1aeedd2c"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "mlb = MultiLabelBinarizer(classes=list(labelVocab.values()))\n",
        "mlb.fit([[each] for each in list(labelVocab.values())])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ab9f06d-10e8-471d-b99e-2d6117ef8abf",
      "metadata": {
        "id": "3ab9f06d-10e8-471d-b99e-2d6117ef8abf"
      },
      "source": [
        "2. Model development"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "429e6394-9904-4c6e-b9d9-ec2c755c823a",
      "metadata": {
        "id": "429e6394-9904-4c6e-b9d9-ec2c755c823a"
      },
      "outputs": [],
      "source": [
        "training_stats = []\n",
        "\n",
        "def training(e, train_dataloader, valid_dataloader):\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # Reset the loss at each epoch\n",
        "    temp_loss, temp_f1 = [], []\n",
        "    train_f1, train_loss = [], []\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    cnt = 0\n",
        "    count = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        cnt +=1\n",
        "        age_ids, input_ids, mod_ids, del_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
        "        targets = torch.tensor(mlb.transform(targets.numpy()), dtype=torch.float32)\n",
        "\n",
        "        ## Load batch to gpu\n",
        "        age_ids = age_ids.to(global_params['device'])\n",
        "        input_ids = input_ids.to(global_params['device'])\n",
        "        mod_ids = mod_ids.to(global_params['device'])\n",
        "        del_ids = del_ids.to(global_params['device'])\n",
        "        posi_ids = posi_ids.to(global_params['device'])\n",
        "        segment_ids = segment_ids.to(global_params['device'])\n",
        "        attMask = attMask.to(global_params['device'])\n",
        "        targets = targets.to(global_params['device'])\n",
        "\n",
        "\n",
        "        ## Compute output (loss, logits and attentions scores)\n",
        "        output = model(input_ids, mod_ids, age_ids, del_ids, segment_ids, posi_ids,attention_mask=attMask, labels=targets, output_attentions = True)\n",
        "\n",
        "        loss = output.loss\n",
        "        logits  = output.logits\n",
        "        attentions = output.attentions\n",
        "\n",
        "        if global_params['gradient_accumulation_steps'] >1:\n",
        "            loss = loss/global_params['gradient_accumulation_steps']\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "\n",
        "        temp_loss.append(loss.item())\n",
        "        train_loss.append(loss.item())\n",
        "        nb_tr_examples += input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "        f1, a, b = scoring_for_train(logits, targets)\n",
        "\n",
        "        temp_f1.append(f1)\n",
        "\n",
        "        # Progress update every 50 batches\n",
        "        if step % 50 == 0:\n",
        "            print(\"epoch: {}\\t| Cnt: {}\\t| Loss: {}\\t| f1 score: {}\".format(e, cnt, np.mean(temp_loss), np.mean(temp_f1)))\n",
        "            temp_loss, temp_f1 = [], []\n",
        "\n",
        "\n",
        "        if (step + 1) % global_params['gradient_accumulation_steps'] == 0:\n",
        "\n",
        "            optim.step()\n",
        "            optim.zero_grad()\n",
        "\n",
        "        train_f1.append(f1)\n",
        "        count+=1\n",
        "\n",
        "\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - start)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Average training loss: {0:.2f}\".format(np.mean(train_loss)))\n",
        "    print(\"Average training score: {0:.2f}\".format(np.mean(train_f1)))\n",
        "    print(\"Training epoch took: {:}\".format(training_time))\n",
        "\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    valid_f1, valid_roc, valid_loss, valid_time = evaluation(valid_dataloader)\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append({'epoch': e+1,\n",
        "                               'Training Loss': np.mean(train_loss) ,\n",
        "                               'Training Time' : training_time,\n",
        "                               'Average Train f1 score' : np.mean(train_f1),\n",
        "                               'Validation Loss': np.mean(valid_loss),\n",
        "                               'Validating Time' : valid_time,\n",
        "                               'Average Valid f1 score' : np.mean(valid_f1),\n",
        "                               'Average Valid ROC' : np.mean(valid_roc)\n",
        "                                })\n",
        "\n",
        "    return valid_f1, train_f1, valid_loss, train_loss, valid_roc, training_stats\n",
        "\n",
        "\n",
        "def evaluation(data):\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    model.eval()\n",
        "    y = []\n",
        "    y_label = []\n",
        "    val_loss, total_f1, total_roc = [], [], []\n",
        "    val_loss_tot, tot_f1, tot_roc = [], [], []\n",
        "\n",
        "    for step, batch in enumerate(data):\n",
        "\n",
        "        age_ids, input_ids, mod_ids, del_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
        "        targets = torch.tensor(mlb.transform(targets.numpy()), dtype=torch.float32)\n",
        "\n",
        "\n",
        "        age_ids = age_ids.to(global_params['device'])\n",
        "        input_ids = input_ids.to(global_params['device'])\n",
        "        mod_ids = mod_ids.to(global_params['device'])\n",
        "        del_ids = del_ids.to(global_params['device'])\n",
        "        posi_ids = posi_ids.to(global_params['device'])\n",
        "        segment_ids = segment_ids.to(global_params['device'])\n",
        "        attMask = attMask.to(global_params['device'])\n",
        "        targets = targets.to(global_params['device'])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, mod_ids, age_ids, del_ids, segment_ids, posi_ids,attention_mask=attMask, labels=targets, output_attentions=True)\n",
        "\n",
        "        val_loss.append(outputs.loss.item())\n",
        "\n",
        "        logits = outputs.logits.cpu()\n",
        "        targets = targets.cpu()\n",
        "\n",
        "        y_label.append(targets)\n",
        "        y.append(logits)\n",
        "\n",
        "\n",
        "    y_label = torch.cat(y_label, dim=0)\n",
        "    y = torch.cat(y, dim=0)\n",
        "\n",
        "    tot_f1, tot_roc, output, label = precision_valid(y, y_label)\n",
        "\n",
        "    # Measure how long this epoch took.\n",
        "    validation_time = format_time(time.time() - start)\n",
        "\n",
        "    print(\"Validation Loss: {0:.2f}\".format(np.mean(val_loss)))\n",
        "    print(\"Validation took: {:}\".format(validation_time))\n",
        "    print()\n",
        "\n",
        "    #output_path = 'BEHRT/Early_integration/Tasks/Output/binary_clf'\n",
        "    #os.makedirs(output_path, exist_ok=True)\n",
        "    #torch.save(model.state_dict(), optim.state_dict()), output_path+'/checkpoint.pt')\n",
        "    #session.report({'loss':np.mean(val_loss), 'f1_score':np.mean(tot_f1)}, checkpoint=checkpoint)\n",
        "\n",
        "    return tot_f1, tot_roc, val_loss, validation_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b76f5e0b-f4a4-4dbb-978b-194f56d42bdf",
      "metadata": {
        "id": "b76f5e0b-f4a4-4dbb-978b-194f56d42bdf"
      },
      "outputs": [],
      "source": [
        "def train_BEHRT(config, epochs=range(2), inputs='inputs_quantiles_preprocessed_100_therap_removed_tolist', age = \"age_100_therap_removed_into_chunks_tolist\",\n",
        "         modalities = 'modalities_100_therap_removed_into_chunks_tolist', delay ='delays_100_therap_removed_into_chunks_tolist'):\n",
        "\n",
        "    set_seed()\n",
        "    train_skf = pd.concat([train, valid])\n",
        "    X_train_skf = train_skf['inputs_quantiles_preprocessed_100_therap_removed_tolist']\n",
        "    y_train_skf = train_skf['label']\n",
        "\n",
        "    n = 5\n",
        "    skf = StratifiedKFold(n_splits = n)\n",
        "\n",
        "\n",
        "    for i, (train_idx, cross_val_idx) in enumerate(skf.split(X_train_skf, y_train_skf)):\n",
        "        train_df = train_skf.iloc[train_idx]\n",
        "        train_df.index = range(train_df.shape[0])\n",
        "        train_df.patid = range(train_df.shape[0])\n",
        "\n",
        "        cv_df = train_skf.iloc[cross_val_idx]\n",
        "        cv_df.index = range(cv_df.shape[0])\n",
        "        cv_df.patid = range(cv_df.shape[0])\n",
        "\n",
        "        ## Load train and valid data\n",
        "\n",
        "        X_tr = train_df[inputs]\n",
        "        y_tr = train_df['label']\n",
        "\n",
        "        Dset = NextVisit(token2idx=tokenVocab, label2idx=labelVocab, mod2idx=modalitiesVocab, age2idx=ageVocab, del2idx=delayVocab, dataframe=train_df, max_len=global_params['max_len_seq'], code= inputs, delay = delay, age = age, mod=modalities)\n",
        "        hp_generator = {'batch_size': config['batch_size'], 'balanced': 'balanced', 'shuffle':True}\n",
        "        train_data = DataLoader(dataset=Dset, batch_size=config['batch_size'],  sampler = StratifiedSampler(X_tr, y_tr, batch_size=config['batch_size']) , **kwargs)\n",
        "\n",
        "        Dset = NextVisit(token2idx = tokenVocab, label2idx=labelVocab, mod2idx=modalitiesVocab, age2idx=ageVocab, del2idx=delayVocab, dataframe=cv_df, max_len=global_params['max_len_seq'], code= inputs, delay = delay, age = age, mod=modalities)\n",
        "        valid_data = DataLoader(dataset = Dset, batch_size=config['batch_size'],  shuffle = True, **kwargs)\n",
        "\n",
        "        model, optimizer = define_model(config)\n",
        "        model.to(global_params['device'])\n",
        "\n",
        "    ## To restore checkpoint, use 'checkpoint.get_checkpoint()'\n",
        "        loaded_checkpoint = session.get_checkpoint()\n",
        "        if loaded_checkpoint:\n",
        "            with loaded_checkpoint.as_directory() as loaded_checkpoint_dir:\n",
        "                model_state, optimizer_state = torch.load(os.pathe.join(loaded_checkpoint_dir, \"checkpoint_test.pt\"))\n",
        "\n",
        "        model.load_state_dict(model_state)\n",
        "        optim.load_state_dict(optimizer_state)\n",
        "\n",
        "        for e in range(epochs):\n",
        "            valid_f1, train_f1, valid_loss, train_loss, valid_roc, training_stats = training(e, train_data, valid_data)\n",
        "            mean_f1 = np.mean(valid_f1)\n",
        "\n",
        "            if mean_f1 > best_pre:\n",
        "                ## Save a trained model\n",
        "                model_to_save = model.module if hasattr(model, 'module') else model\n",
        "                output_module_file = os.path.join(global_params['output_dir'])\n",
        "                create_folder(global_params['output_dir'])\n",
        "                torch.save(model_to_save.state_dict(), output_model_file)\n",
        "                best_pre = mean_f1\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "21db18ce-52a2-45f2-84ea-e0e1179166c6",
      "metadata": {
        "id": "21db18ce-52a2-45f2-84ea-e0e1179166c6"
      },
      "outputs": [],
      "source": [
        "def test_BEHRT(best_result, inputs='inputs_curve_intersections_preprocessed_100_therap_removed_tolist', age= \"age_100_therap_removed_into_chunks_tolist\",\n",
        "              delay='delays_100_therap_removed_into_chunks_tolist', mod = 'modalities_100_therap_removed_into_chunks_tolist'):\n",
        "\n",
        "    Dset = NextVisit(token2idx=tokenVocab, label2idx=labelVocab, mod2idx=modalitiesVocab, age2idx=ageVocab, del2idx=delayVocab, dataframe=test, max_len=global_params['max_len_seq'],\n",
        "                     code = inputs, delay = delay, age = age, mod=mod )\n",
        "    testload = DataLoader(dataset=Dset, batch_size=global_params['batch_size'],  shuffle = False, **kwargs)\n",
        "\n",
        "    model = define_model(best_result)\n",
        "    model.to(global_params['device'])\n",
        "    checkpoint_path = os.path.join(best_result.checkpoint.to_directory(), 'checkpoint_test.pt')\n",
        "\n",
        "    model_state, optimizer_state = torch.load(checkpoint_path)\n",
        "    model.load_state_dict(model_state)\n",
        "\n",
        "    y, y_label = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step, batch in enumerate(testload):\n",
        "            model.eval()\n",
        "\n",
        "            age_ids, input_ids, mod_ids, del_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
        "            targets = torch.tensor(mlb.transform(targets.numpy()), dtype=torch.float32)\n",
        "\n",
        "            age_ids = age_ids.to(global_params['device'])\n",
        "            mod_ids = mod_ids.to(global_params['device'])\n",
        "            del_ids = del_ids.to(global_params['device'])\n",
        "            input_ids = input_ids.to(global_params['device'])\n",
        "            posi_ids = posi_ids.to(global_params['device'])\n",
        "            segment_ids = segment_ids.to(global_params['device'])\n",
        "            attMask = attMask.to(global_params['device'])\n",
        "            targets = targets.to(global_params['device'])\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(input_ids, mod_ids, age_ids, del_ids, segment_ids, posi_ids,attention_mask=attMask, labels=targets, output_attentions=True)\n",
        "\n",
        "            logits = outputs.logits.cpu()\n",
        "            targets = targets.cpu()\n",
        "\n",
        "            y_label.append(targets)\n",
        "            y.append(logits)\n",
        "\n",
        "\n",
        "    y_label = torch.cat(y_label, dim=0)\n",
        "    y = torch.cat(y, dim=0)\n",
        "\n",
        "     # Compute ROC curve and ROC area for each class\n",
        "    f1, roc, recall, precision, output, label, = precision_test(y, y_label)\n",
        "\n",
        "    print(\"Best trial test set scores: f1: {}, roc: {}, recall: {} and precision: {}.\".format(f1, roc, recall, precision))\n",
        "\n",
        "    return y, y_label\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68c0f8f2-54c3-44c0-b119-d9185898c5f0",
      "metadata": {
        "id": "68c0f8f2-54c3-44c0-b119-d9185898c5f0"
      },
      "outputs": [],
      "source": [
        "## Training with best config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d529ce6e-2f76-4315-a4fa-683cca69709e",
      "metadata": {
        "id": "d529ce6e-2f76-4315-a4fa-683cca69709e"
      },
      "outputs": [],
      "source": [
        "best_config = best_result.config\n",
        "hf._dump_pkl(best_config, 'best_config')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__==\"__main__\":\n",
        "\n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "\n",
        "    # Let's set a seed value to make this reproducible\n",
        "    set_seed()\n",
        "    train_skf = pd.concat([train, valid])\n",
        "\n",
        "    train_f1_list, train_loss_list = [], []\n",
        "    valid_f1_list, valid_f1_list_tot, valid_loss_list, valid_roc_list = [], [], [], []\n",
        "    best_pre = 0.0\n",
        "    a = 0\n",
        "    n = 5\n",
        "    kf = KFold(n_splits=n)\n",
        "    train_results, valid_results, valid_results_tot, train_loss, valid_loss, valid_results_roc, training_stats_list  = [], [], [], [], [], [], []\n",
        "\n",
        "\n",
        "    for train_idx, cross_val_idx in kf.split(train_skf):\n",
        "\n",
        "        print(\"\",end=\"\\n\\n\")\n",
        "        print(f\"Generating Inputs for fold {a}\")\n",
        "        print(\"==\"*20)\n",
        "\n",
        "        train_df = train_skf.iloc[train_idx]\n",
        "        train_df.index = range(train_df.shape[0])\n",
        "        train_df.patid = range(train_df.shape[0])\n",
        "\n",
        "        cv_df = train_skf.iloc[cross_val_idx]\n",
        "        cv_df.index = range(cv_df.shape[0])\n",
        "        cv_df.patid = range(cv_df.shape[0])\n",
        "\n",
        "\n",
        "        X_tr = train_df['inputs_quantiles_cleaned']\n",
        "        y_tr = train_df['label']\n",
        "\n",
        "        Dset = FinetuneDataset(token2idx=tokenVocab, label2idx=labelVocab, mod2idx=modalitiesVocab, age2idx=ageVocab, del2idx=delayVocab, dataframe=train_df, max_len=global_params['max_len_seq'], code='inputs_quantiles_cleaned', delay ='delays' )\n",
        "        hp_generator = {'batch_size': global_params['batch_size'], 'balanced': 'balanced', 'shuffle':True}\n",
        "        train_data = DataLoader(dataset=Dset, batch_size=global_params['batch_size'],  sampler = StratifiedSampler(X_tr, y_tr, batch_size=global_params['batch_size']) , **kwargs)\n",
        "\n",
        "\n",
        "        valid_Dset = FinetuneDataset(token2idx=tokenVocab, label2idx=labelVocab, mod2idx=modalitiesVocab, age2idx=ageVocab, del2idx=delayVocab, dataframe=cv_df, max_len=global_params['max_len_seq'], code='inputs_quantiles_cleaned', delay ='delays')\n",
        "        valid_data = DataLoader(dataset= Dset, batch_size=global_params['batch_size'],  shuffle = True, **kwargs)\n",
        "\n",
        "        model, optim = define_model(best_config)\n",
        "\n",
        "        for e in range(200):\n",
        "\n",
        "            valid_f1, train_f1, valid_loss, train_loss, valid_roc, training_stats =  training(e, train_data, valid_data)\n",
        "\n",
        "            train_f1_list.append(np.mean(train_f1))\n",
        "            train_loss_list.append(np.mean(train_loss))\n",
        "            valid_f1_list.append(np.mean(valid_f1))\n",
        "            valid_roc_list.append(np.mean(valid_roc))\n",
        "            valid_loss_list.append(np.mean(valid_loss))\n",
        "\n",
        "            mean_f1 = np.mean(valid_f1)\n",
        "\n",
        "            if mean_f1 > best_pre:\n",
        "                # Save a trained model\n",
        "                print(\"** ** * Saving fine - tuned model ** ** * \")\n",
        "                model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "                output_model_file = os.path.join(global_params['output_dir'],global_params['best_name'])\n",
        "                create_folder(global_params['output_dir'])\n",
        "\n",
        "                torch.save(model_to_save.state_dict(), output_model_file)\n",
        "                best_pre = mean_f1\n",
        "            print('valid score: {}'.format(np.mean(valid_f1)))\n",
        "\n",
        "\n",
        "            print()\n",
        "\n",
        "        a+=1\n",
        "\n",
        "    train_results.append(train_f1)\n",
        "    valid_results.append(valid_f1)\n",
        "    train_loss.append(train_loss)\n",
        "    valid_loss.append(valid_loss)\n",
        "    valid_results_roc.append(valid_roc)\n",
        "    training_stats_list.append(training_stats)\n",
        "\n",
        "\n",
        "    end = time.time()\n",
        "    print(\"\")\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(end-start)))\n"
      ],
      "metadata": {
        "id": "fuAd12uB8FPv"
      },
      "id": "fuAd12uB8FPv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "641fe13b-5d5b-4978-b52b-40cf7f540e3a",
      "metadata": {
        "id": "641fe13b-5d5b-4978-b52b-40cf7f540e3a"
      },
      "source": [
        "#### Evaluate model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2ba8c9e1-53a8-4acf-b09f-24c5334a0126",
      "metadata": {
        "id": "2ba8c9e1-53a8-4acf-b09f-24c5334a0126"
      },
      "source": [
        "1. Plot model history  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c11d7733-29a1-4f23-a1ae-80e716260850",
      "metadata": {
        "id": "c11d7733-29a1-4f23-a1ae-80e716260850"
      },
      "outputs": [],
      "source": [
        "df_stats = display_training_stats(training_stats)\n",
        "df_stats.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bcb6c3d6-aa7d-4cf9-93e2-d2b611f63e29",
      "metadata": {
        "id": "bcb6c3d6-aa7d-4cf9-93e2-d2b611f63e29"
      },
      "outputs": [],
      "source": [
        "plot_history(training_stats, 200)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfa8b36c-c824-42f2-83f4-6db21ea055c1",
      "metadata": {
        "id": "bfa8b36c-c824-42f2-83f4-6db21ea055c1"
      },
      "source": [
        "2. Display results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aa72689-99fe-4e39-a9d0-edc9cf82de94",
      "metadata": {
        "id": "6aa72689-99fe-4e39-a9d0-edc9cf82de94",
        "outputId": "c8b51970-00e1-474b-ce6c-dfa146b3399c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicting labels for 4,545 test sequences...\n",
            "Validation Loss: 0.27\n",
            "Validation took: 0:00:09\n",
            "\n",
            "Test f1 score = 0.7349955368785289 and Test roc = 0.7656285894785205\n"
          ]
        }
      ],
      "source": [
        "# Prediction on test set ## 2nd run , cv and cleaned\n",
        "\n",
        "print('Predicting labels for {:,} test sequences...'.format(len(test)))\n",
        "f1, roc, loss, test_time = evaluation(testload)\n",
        "print('Test f1 score = {} and Test roc = {}'.format(np.mean(f1), np.mean(roc)))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}