{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "78498f0b-e7b4-4fbe-9e40-827fdf2025eb",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a24a8dd-3d4b-47fd-b884-6924e46f061a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-28 04:47:15.593041: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-28 04:47:16.746643: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-28 04:47:16.746762: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-28 04:47:16.746774: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('BEHRT/Early_integration/')\n",
    "sys.path.append('BEHRT/')\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "from Utils.common import *\n",
    "from Utils.pytorch import load_model\n",
    "from Utils.utils import age_vocab, input_vocab, mod_vocab, delay_vocab\n",
    "from Utils.optimiser import adam\n",
    "import random\n",
    "import pytorch_pretrained_bert as Bert\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import pandas as pd\n",
    "import sklearn.metrics as skm\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from Utils.dataLoader_utils import seq_padding, random_mask, code2index,label2index, position_idx, index_seg\n",
    "from Utils.add_endpoints import add_endp\n",
    "from Utils.handle_file import handle_file\n",
    "from Utils.FineTuneLoader import FinetuneDataset\n",
    "from Utils.utils_for_classification import *\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "hf = handle_file()\n",
    "from Utils.preprocessing import cal_age, cal_therapies, spark_init, cal_subtherapies, int_to_list, flatten_list, int_to_str, cal_therapies_fr, divide_frame\n",
    "from datetime import datetime, timedelta\n",
    "from torch.utils.data import Dataset, DataLoader, Sampler\n",
    "import sklearn\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import math\n",
    "import datetime \n",
    "from sklearn.metrics import roc_auc_score\n",
    "from Utils import optimiser\n",
    "from Utils.dataLoader_utils import ImbSampler, OverSampler\n",
    "from Models.BertForClassification import BertForClassification\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore')\n",
    "from collections import Counter\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd2d10a-8a1e-4c27-9da8-53b4c6ae1611",
   "metadata": {},
   "source": [
    "#### 1. Training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f47beb-4636-4dd5-a2cb-50682d8e5fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(path, model):\n",
    "    \"\"\"Load pretrained weigths to  the model\"\"\"\n",
    "    # load pretrained model and update weights\n",
    "    pretrained_dict = torch.load(path).state_dict()\n",
    "    model_dict = model.state_dict()\n",
    "    \n",
    "    # 1. filter out unnecessary keys\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    \n",
    "    # 2. overwrite entries in the existing state dict\n",
    "    model_dict.update(pretrained_dict)\n",
    "    \n",
    "    # 3. load the new state dict\n",
    "    model.load_state_dict(model_dict)\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8b0fd5-3da5-445c-bfa1-e35e9a256223",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats = list()        \n",
    "def training(model, optim, config, e, train_dataloader, valid_dataloader, global_params, mlb):\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # Reset the loss at each epoch\n",
    "    temp_loss, temp_f1 = [], []\n",
    "    train_f1, train_loss = [], []\n",
    "    train_targets, train_masks = [], []\n",
    "    \n",
    "    \n",
    "    ## Count the number of batches\n",
    "    cnt = 0\n",
    "    count = 0\n",
    "    # Training\n",
    "    model.train()\n",
    "    \n",
    "\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "         \n",
    "        cnt +=1\n",
    "        if config.age_in_inputs:\n",
    "            input_ids, mod_ids, NPI_ids, del_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
    "            age_ids = None\n",
    "            del_ids = del_ids.to(global_params['device'])\n",
    "            \n",
    "        if config.delays_in_inputs:\n",
    "            input_ids, mod_ids, NPI_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
    "            age_ids = None\n",
    "            del_ids = None\n",
    "        else:\n",
    "            age_ids, input_ids, mod_ids, NPI_ids, del_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
    "            age_ids = age_ids.to(global_params['device'])\n",
    "            del_ids = del_ids.to(global_params['device'])\n",
    "             \n",
    "        targets = torch.tensor(mlb.transform(targets.numpy()), dtype=torch.float32)\n",
    "        \n",
    "        ## Load batch to gpu \n",
    "        input_ids = input_ids.to(global_params['device'])\n",
    "        mod_ids = mod_ids.to(global_params['device'])\n",
    "        posi_ids = posi_ids.to(global_params['device'])\n",
    "        segment_ids = segment_ids.to(global_params['device'])\n",
    "        attMask = attMask.to(global_params['device'])\n",
    "        NPI_ids = NPI_ids.to(global_params['device'])\n",
    "        targets = targets.to(global_params['device'])\n",
    "        \n",
    "        \n",
    "        ## Compute output (loss, logits and attentions scores) \n",
    "        output = model(input_ids = input_ids,modalities_ids= mod_ids, age_ids=age_ids,delays_ids= del_ids, seg_ids=segment_ids, posi_ids=posi_ids, NPI_ids=NPI_ids, attention_mask=attMask, labels=targets, output_attentions = True)\n",
    "        \n",
    "        loss = output.loss\n",
    "        logits  = output.logits\n",
    "        attentions = output.attentions\n",
    "        \n",
    "        if config['gradient_accumulation_steps'] > 1:\n",
    "            loss = loss/config['gradient_accumulation_steps']\n",
    "        \n",
    "    \n",
    "        temp_loss.append(loss.item())\n",
    "        train_loss.append(loss.item())\n",
    "        \n",
    "        auc, aps, f1, recall, precision, _, _ = scores(logits, targets, training=True)\n",
    "   \n",
    "        temp_f1.append(f1)\n",
    "        \n",
    "        # Progress update every 100 batches\n",
    "        if step % 100 == 0:\n",
    "            print(\"epoch: {}\\t| Cnt: {}\\t| Loss: {}\\t| f1 score: {}\".format(e, cnt, np.mean(temp_loss), np.mean(temp_f1)))\n",
    "            temp_loss, temp_f1 = [], []\n",
    "        \n",
    "        if (step + 1) % config['gradient_accumulation_steps'] == 0:\n",
    "            optim.zero_grad()\n",
    "            loss.sum().backward()\n",
    "            optim.step()\n",
    "        \n",
    "        train_f1.append(f1)\n",
    "        count+=1\n",
    "        train_targets.append(targets)\n",
    "        train_masks.append(attMask)\n",
    "        logits = logits.cpu()\n",
    "        targets = targets.cpu()\n",
    "        loss = loss.cpu()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    train_hidden_states = output.hidden_states[1:]\n",
    " \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - start)\n",
    "                   \n",
    "    print(\"\")\n",
    "    print(\"Average training loss: {0:.2f}\".format(np.mean(train_loss)))\n",
    "    print(\"Average training score: {0:.2f}\".format(np.mean(train_f1)))\n",
    "    print(\"Training epoch took: {:}\".format(training_time))\n",
    "    \n",
    "          \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "      \n",
    "    valid_f1, valid_auc, valid_aps, valid_recall, valid_precision, valid_loss, valid_time, valid_hidden_states, valid_masks = evaluating(model, config, valid_dataloader, global_params, mlb)\n",
    "    \n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append({'epoch': e+1, \n",
    "                               'Training Loss': np.mean(train_loss) ,\n",
    "                               'Training Time' : training_time,\n",
    "                               'Average Train f1 score' : np.mean(train_f1),\n",
    "                               'Validation Loss': np.mean(valid_loss), \n",
    "                               'Validating Time' : valid_time, \n",
    "                               'Average Valid f1 score' : np.mean(valid_f1),\n",
    "                               'Average Valid ROC-AUC score' : np.mean(valid_auc), \n",
    "                               'Average Valid precision' : np.mean(precision),\n",
    "                               'Average Valid recall' : np.mean(recall),\n",
    "                                })  \n",
    "    \n",
    "    \n",
    "    return valid_f1, train_f1, valid_loss, train_loss, valid_auc,  valid_aps, training_stats, train_hidden_states, valid_hidden_states, train_targets, train_masks, valid_masks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b196abe0-431b-4b1a-9a05-5307278e8611",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluating(model, config, data, global_params, mlb):\n",
    "                               \n",
    "    start = time.time()\n",
    "                               \n",
    "    model.eval()\n",
    "    y_pred, y_true = [], []\n",
    "    loss, masks = [], []\n",
    "    \n",
    "    for step, batch in enumerate(data):\n",
    "        \n",
    "        if config.age_in_inputs:\n",
    "            input_ids, mod_ids, del_ids, NPI_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
    "            age_ids = None\n",
    "            del_ids = del_ids.to(global_params['device'])\n",
    "            \n",
    "        if config.delays_in_inputs:\n",
    "\n",
    "            input_ids, mod_ids, NPI_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
    "            age_ids = None\n",
    "            del_ids = None\n",
    "        else:\n",
    "            age_ids, input_ids, mod_ids,  NPI_ids, del_ids, posi_ids, segment_ids, attMask, targets, _ = batch\n",
    "            age_ids = age_ids.to(global_params['device'])\n",
    "            del_ids = del_ids.to(global_params['device'])\n",
    "             \n",
    "        targets = torch.tensor(mlb.transform(targets.numpy()), dtype=torch.float32)\n",
    "    \n",
    "        ## Load batch to gpu \n",
    "        input_ids = input_ids.to(global_params['device'])\n",
    "        mod_ids = mod_ids.to(global_params['device'])\n",
    "        posi_ids = posi_ids.to(global_params['device'])\n",
    "        segment_ids = segment_ids.to(global_params['device'])\n",
    "        attMask = attMask.to(global_params['device'])\n",
    "        NPI_ids = NPI_ids.to(global_params['device'])\n",
    "        targets = targets.to(global_params['device'])\n",
    "        \n",
    "      \n",
    "        with torch.no_grad():\n",
    "            outputs = model(input_ids=input_ids, modalities_ids= mod_ids, age_ids= age_ids, delays_ids=del_ids, seg_ids=segment_ids,posi_ids= posi_ids, NPI_ids=NPI_ids, attention_mask=attMask, labels=targets, output_attentions=True)\n",
    "        \n",
    "        #loss.append(outputs.loss.sum().item())\n",
    "        loss.append(outputs.loss.item())\n",
    "        \n",
    "        \n",
    "        logits = outputs.logits.cpu()\n",
    "        targets = targets.cpu()\n",
    "        \n",
    "        y_true.append(targets)\n",
    "        y_pred.append(logits)\n",
    "        \n",
    "        masks.append(attMask)\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "    hidden_states = outputs.hidden_states[1:]\n",
    "    \n",
    "    y_true = torch.cat(y_true, dim=0)\n",
    "    y_pred = torch.cat(y_pred, dim=0) \n",
    "    \n",
    "    auc, aps, f1, recall, precision, logits, labels = scores(y_pred, y_true)\n",
    "   \n",
    "    # Measure how long this epoch took.\n",
    "    val_time = format_time(time.time() - start)\n",
    "    \n",
    "    \n",
    "    print(\"Validation Loss: {0:.2f}\".format(np.mean(loss)))\n",
    "    print(\"Validation took: {:}\".format(val_time))\n",
    "    print()\n",
    "\n",
    "    return f1, auc, aps, recall, precision, loss, val_time, hidden_states, masks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd337188-4d02-4014-9ca2-7ac4c670566c",
   "metadata": {},
   "source": [
    "#### Hyperparameters tuning with Weights & Biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae5d53c-cceb-4938-af9c-fc7df79fc41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "\n",
    "os.environ['https_proxy'] = \"\"\n",
    "os.environ['http_proxy'] = \"\"\n",
    "os.environ['no_proxy']= \"\"\n",
    "os.environ['HTTPS_PROXY']= \"\"\n",
    "os.environ['HTTP_PROXY']= \"\"\n",
    "os.environ['NO_PROXY']= \"\"\n",
    "\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = 'Tabular_BEHRT/Tasks/Binary_classification.ipynb'\n",
    "os.environ['WANDB_API_KEY'] = \"\"\n",
    "os.environ['WANDB_NAME'] = \"Binary_classification\"\n",
    "os.environ['WANDB_NOTES'] = \"Hyperparameters tuning using wandb for binary clf: Bayesian and hyperband scheduler\"\n",
    "os.environ['WANDB_AGENT_MAX_INITIAL_FAILURES']='0'\n",
    "os.environ['WANDB__SERVICE_WAIT'] =\"300\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\"\n",
    "\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a691f96-f00a-4748-9f42-2cb9d22a8033",
   "metadata": {},
   "source": [
    "#### Run Binary classification \n",
    "1. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5adc5959-e763-4719-8d36-25c616420c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_model_path =  \"\"   # pretrained MLM path\n",
    "\n",
    "file_config = { 'data':\"\",\n",
    "                'labels' : \"\" , \n",
    "                'test_idxs' : \"\",\n",
    "\n",
    "                }\n",
    "\n",
    "global_params = {\n",
    "    'device': device,\n",
    "    'output_dir': \"\", # output folder\n",
    "    'best_name': 'best_clf',   # output model name\n",
    "    'max_len_seq': 512, #100,\n",
    "    'max_age': 110,\n",
    "    'max_delay': 30, ## = 30years\n",
    "    'age_month': 12,\n",
    "    'delay_month': 0.25,\n",
    "    'age_year': False,\n",
    "    'age_symbol': None,\n",
    "    'min_visit': 5,\n",
    "    \n",
    "}\n",
    "\n",
    "train_params = {\n",
    "    'use_cuda': True,\n",
    "    'max_len_seq': global_params['max_len_seq'],\n",
    "    'device': device,\n",
    "}\n",
    "\n",
    "feature_dict = {\n",
    "    'word':True,\n",
    "    'seg':True,\n",
    "    'age':False,\n",
    "    'modalities': True,\n",
    "    'delays': False,\n",
    "    'position': True, \n",
    "    'NPI' : True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294488fe-1f32-4538-9bfc-759d10af5a9a",
   "metadata": {},
   "source": [
    "2. Import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9540195c-46f5-43b7-94df-983c6100cd53",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Import train data \"\"\"\n",
    "data =  hf._load_pkl(file_config['data'])\n",
    "\n",
    "\"\"\"Import labels \"\"\"\n",
    "labels = hf._load_pkl(file_config['labels'])\n",
    "\n",
    "\"\"\"Merge label and data \"\"\"\n",
    "data = pd.merge(data, labels, on=['Num_dossier'])\n",
    "\n",
    "# Display 3 first rows\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cef4909e-bbee-4d65-aa79-8015f9a65c26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10029"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_censored = data.loc[((data['delay_dfs_surg'] < 35.9951) & (data['label'] == 0))]['Num_dossier'].values.tolist()\n",
    "index_relapse_1y = data.loc[((data['status_dfs'] == 1) & (data['delay_dfs_surg'] < 11.9983))]['Num_dossier'].values.tolist()\n",
    "\n",
    "data = data.loc[~data['Num_dossier'].isin(index_censored_3y+index_relapse_1y)]#['status_dfs_surg_3y'].value_counts()#.sum()  ## 8124\n",
    "print(data.shape, len(index_censored_3y), len(index_relapse_1y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8991f600-12b3-46f1-bba8-a44351cb55ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We lost 0 patient\n"
     ]
    }
   ],
   "source": [
    "# remove patients with visits less than min visit\n",
    "previous_shape = data.shape[0]\n",
    "\n",
    "data['length'] = data['inputs_normal_range_and_delta_age_and_delays_preprocessed'].apply(lambda x: len([i for i in range(len(x)) if x[i] == 'SEP']))\n",
    "data = data[data['length'] >= global_params['min_visit']]\n",
    "#data = data.reset_index(drop=True)\n",
    "\n",
    "print(\"We lost {} patient\".format(previous_shape - data.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ff1b5cd-e187-49c8-b148-cb68eae6b9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## use the vocabulary from the entire history dataset\n",
    "\n",
    "whole_seq = hf._load_pkl('')\n",
    "tokenVocab, _ = input_vocab(inputs = whole_seq.inputs, symbol=global_params['age_symbol'])\n",
    "modalitiesVocab, _ = mod_vocab(whole_seq.modalities, symbol=global_params['age_symbol'])\n",
    "NPIVocab = {'PAD':0, 'UNK':1, 'VPPG':2, 'PPG':3, 'MPGI':4, 'MPGII':5, 'GPG':6, 'EPG':7}\n",
    "\n",
    "# Binary classification \n",
    "labelVocab = {0:0, 1:1}\n",
    "\n",
    "del whole_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd1ed32d-08af-404b-8c4b-7735a076dbfa",
   "metadata": {},
   "source": [
    "4. Prepare data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635d3df5-8785-4d39-ab49-19d8217657e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_idxs = hf._load_pkl(file_config['test_idxs'])\n",
    "\n",
    "test = data.loc[data['Num_dossier'].isin(test_idxs.values.tolist())]\n",
    "train =  data.loc[~data['Num_dossier'].isin(test_idxs.values.tolist())] \n",
    "\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d2944d57-9d7a-43dd-94ab-463a5c87d12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['patid'] = range(len(train))\n",
    "test['patid'] = range(len(test))\n",
    "\n",
    "train.index =  range(len(train))\n",
    "test.index =  range(len(test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc00590-e26b-4490-9791-d5b12c425181",
   "metadata": {},
   "source": [
    "3. Config and utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cdae6c72-8d3a-44c5-a38b-c2b7702980c7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenVocab' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/b1/jf_78yv56k71f696t4mbm2jw0000gn/T/ipykernel_46935/4115158770.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;34m'attention_probs_dropout_prob'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;34m'hidden_act'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'gelu'\u001b[0m\u001b[0;34m,\u001b[0m                                                 \u001b[0;31m# The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0;34m'vocab_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenVocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m                                                  \u001b[0;31m# number of disease + symbols for word embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;34m'seg_vocab_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m                                                             \u001b[0;31m# number of vocab for seg embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;34m'modalities_vocab_size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodalitiesVocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenVocab' is not defined"
     ]
    }
   ],
   "source": [
    "## Config for the task with sweep (wandb)\n",
    "\n",
    "config = {'method' : 'random', \n",
    "                'metric' : {'name' : 'valid_aps', 'goal': 'maximize'},\n",
    "                'parameters' : {'hidden_size': {'value': 288},       # word embedding and seg embedding hidden size\n",
    "                'intermediate_size': {'value': 210},                # the size of the \"intermediate\" layer in the transformer encoder\n",
    "                'hidden_dropout_prob': {'values': [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]},                    # dropout rate\n",
    "                'num_hidden_layers': {'value': 5},                   # number of multi-head attention layers required\n",
    "                'num_attention_heads': {'value': 6},                  # number of attention heads\n",
    "                'attention_probs_dropout_prob': {'values': [0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]},\n",
    "                'hidden_act': {'value' :'gelu'},                                                 # The non-linear activation function in the encoder and the pooler \"gelu\", 'relu', 'swish' are supported\n",
    "                'vocab_size': {'value': len(tokenVocab)},                                                  # number of disease + symbols for word embedding\n",
    "                'seg_vocab_size': {'value':2},                                                             # number of vocab for seg embedding\n",
    "                'modalities_vocab_size': {'value':len(modalitiesVocab)}, \n",
    "                'age_vocab_size': {'value':1},                                                 # number of vocab for age embedding\n",
    "                'delays_vocab_size': {'value':1},\n",
    "                'NPI_vocab_size' : {'value': len(NPIVocab)},\n",
    "                'epochs': {'values': [15]},\n",
    "                'max_position_embeddings': {'value': train_params['max_len_seq']},                          # maximum number of tokens\n",
    "                'initializer_range': {'value' : 0.02},                                                    # parameter weight initializer range\n",
    "                'lr' : {'values': [5e-6, 1e-5, 3e-5, 5e-5, 1e-4, 3e-4]},                      \n",
    "                'warmup_proportion': {'value': 0.1}, \n",
    "                'weight_decay' : {'values': [0, 0.0001]},#0.0005, 0.001, 0.005, 0.01, 0.005]}, \n",
    "                'class_weights': {'values': [10, 20, 40, 60]},\n",
    "                'batch_size': {'value': 24},   # integers between 8 and 32 # with evenly-distributed logarithms\n",
    "                'gradient_accumulation_steps':{'distribution': 'int_uniform', 'min':4, 'max':10},\n",
    "                'chunk_size_feed_forward' : {'value':0},\n",
    "                'output_attentions' : {'value':True},\n",
    "                'is_decoder' : {'value':False},\n",
    "                'add_cross_attention':{'value': False},\n",
    "                'layer_norm_eps' : {'value': 1e-5},\n",
    "                'delays_in_inputs': {'value': True},\n",
    "                'activation': {'value':False},\n",
    "                'age_in_inputs': {'value': False}, \n",
    "                'freeze_weights': {'value': False},},     \n",
    "              #  'early_terminate': {'type': 'hyperband', 's': 2, 'eta': 3, 'max_iter': 27},\n",
    "}   \n",
    "\n",
    "from transformers.configuration_utils import PretrainedConfig\n",
    "\n",
    "class BertConfig(PretrainedConfig):\n",
    "    def __init__(self, config):\n",
    "        super(BertConfig, self).__init__(\n",
    "            vocab_size=config.get('vocab_size'),\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_hidden_layers=config.get('num_hidden_layers'),\n",
    "            num_attention_heads=config.get('num_attention_heads'),\n",
    "            intermediate_size=config.get('intermediate_size'),\n",
    "            hidden_act=config.get('hidden_act'),\n",
    "            hidden_dropout_prob=config.get('hidden_dropout_prob'),\n",
    "            attention_probs_dropout_prob=config.get('attention_probs_dropout_prob'),\n",
    "            max_position_embeddings = config.get('max_position_embeddings'),\n",
    "            position_embedding_type = config.get('position_embedding_type'),\n",
    "            initializer_range=config.get('initializer_range'),\n",
    "            \n",
    "        )\n",
    "        self.delays_vocab_size = config.get('delays_vocab_size')\n",
    "        self.modalities_vocab_size = config.get('modalities_vocab_size')\n",
    "        self.age_vocab_size = config.get('age_vocab_size')\n",
    "        self.seg_vocab_size = config.get('seg_vocab_size')\n",
    "        self.NPI_vocab_size = config.get('NPI_vocab_size')\n",
    "        self.output_attentions = config.get('output_attentions')\n",
    "        self.chunk_size_feed_forward = config.get('chunk_size_feed_forward')\n",
    "        self.is_decoder = config.get('is_decoder')\n",
    "        self.layer_norm_eps = config.get('layer_norm_eps')\n",
    "        self.add_cross_attention = config.get('add_cross_attention')\n",
    "        self.activation = config.get('activation')\n",
    "        self.age_in_inputs = config.get('age_in_inputs')\n",
    "        self.delays_in_inputs = config.get('delays_in_inputs')\n",
    "        self.freeze_weights = config.get('freeze_weights')\n",
    "        self.class_weights = config.get('class_weights')\n",
    "        self.freeze_weights = config.get('freeze_weights')\n",
    "        self.gradient_accumulation_steps = config.get('gradient_accumulation_steps')\n",
    "        \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb79caa0-2866-4e22-841f-7d4b4d0b7928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultiLabelBinarizer(classes=[0, 1])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultiLabelBinarizer</label><div class=\"sk-toggleable__content\"><pre>MultiLabelBinarizer(classes=[0, 1])</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultiLabelBinarizer(classes=[0, 1])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer(classes=list(labelVocab.values()))\n",
    "mlb.fit([[each] for each in list(labelVocab.values())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7d86a0-fe67-4cab-8aa5-297dce034604",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Check if the gpu is available \"\"\"\n",
    "if torch.cuda.is_available():\n",
    "   \n",
    "    device = torch.device(\"cuda\")\n",
    "    n_gpus = torch.cuda.device_count()\n",
    "    print(\"There are %d GPU(s) available. \" %torch.cuda.device_count())\n",
    "    print(\"We will use the GPU: {}\".format(torch.cuda.get_device_name(0)))\n",
    "    \n",
    "else:\n",
    "    print(\"No GPU available, using the CPU instead\")\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6903e27e-8d33-438a-ba81-eebc74decec7",
   "metadata": {},
   "source": [
    "4. Sweep settings for hp tuning in W&B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdacdaa-e7dd-4ca5-bf82-c08b160bd764",
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_id = wandb.sweep(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a45ebf2f-3251-435a-94ef-511de41b830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Train the model \n",
    "\n",
    "def define_model(config, pretraining = True):\n",
    "   \n",
    "    conf = BertConfig(config)\n",
    "    model = BertForClassification(conf, num_labels=conf.num_labels, feature_dict=feature_dict, class_weights = conf.class_weights)\n",
    "    if pretraining:\n",
    "        model = load_model(pretrain_model_path, model)   \n",
    "    \n",
    "  #  model = nn.DataParallel(model).cuda()\n",
    "    model = model.cuda()\n",
    "    optim = optimiser.adam(params=list(model.named_parameters()), config=config)\n",
    " \n",
    "    return model, optim\n",
    "\n",
    "def load_data(data, config, sampling=True, X_tr=None, y_tr=None):\n",
    "    \"\"\"\n",
    "    Create a torch DataLoader that can be used to load the data batch by batch.\n",
    "    The batch are balanced using NewBatchOverSampler.\n",
    "    \"\"\"  \n",
    "    Dset = FinetuneDataset(token2idx = tokenVocab, label2idx=labelVocab, mod2idx=modalitiesVocab, \n",
    "                   NPI2idx=NPIVocab, dataframe=data, max_len=global_params['max_len_seq'], \n",
    "                     code='inputs_normal_range_and_delta_age_and_delays_preprocessed',\n",
    "                     mod='modalities_for_twos_and_age_and_delays_preprocessed', NPI='NPI_for_twos_age_delays', age_in_inputs=config.age_in_inputs, delays_in_inputs=config.delays_in_inputs)\n",
    "\n",
    "    if sampling:\n",
    "        return DataLoader(dataset=Dset, batch_sampler=StratifiedSampler(X_tr, y_tr, batch_size=config['batch_size']))\n",
    "    else:\n",
    "     #   return DataLoader(dataset=Dset, batch_size=config.batch_size,  sampler = torch.utils.data.RandomSampler(data.index))\n",
    "        return DataLoader(dataset=Dset, batch_size=config['batch_size'], shuffle=False)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6cdd754-ef9d-4f1c-bb45-68aa440fec2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_per_epochs(model, optim, config, trainload, validload, testload, mlb):\n",
    "  #  empty_cuda()  \n",
    "    train_hidden_states_per_epoch, train_targets_per_epoch, train_mask_per_epoch =  [] , [], []\n",
    "    valid_hidden_states_per_epoch, valid_targets_per_epoch, valid_mask_per_epoch = [], [], []  \n",
    "    valid_f1_list, train_f1_list, valid_loss_list, train_loss_list,  valid_auc_list , valid_aps_list= [], [], [], [], [], []\n",
    "    best_score = 0.0\n",
    "    best_temp_auc = 0.7\n",
    "    \n",
    "    for e in range(config['epochs']):\n",
    "        \n",
    "        valid_f1, train_f1, valid_loss, train_loss, valid_auc, valid_aps, training_stats, train_hidden_states, valid_hidden_states, train_targets, train_masks, valid_masks =  training(model, optim, config, e, trainload, validload, global_params, mlb)\n",
    "        train_hidden_states_per_epoch.append(train_hidden_states)\n",
    "        valid_hidden_states_per_epoch.append(valid_hidden_states)\n",
    "        train_targets_per_epoch.append(train_targets)\n",
    "        train_mask_per_epoch.append(train_masks)\n",
    "        valid_mask_per_epoch.append(valid_masks)\n",
    "\n",
    "        train_f1_list.append(np.mean(train_f1))\n",
    "        train_loss_list.append(np.mean(train_loss))\n",
    "        valid_f1_list.append(np.mean(valid_f1))\n",
    "        valid_auc_list.append(np.mean(valid_auc))\n",
    "        valid_loss_list.append(np.mean(valid_loss))\n",
    "        valid_aps_list.append(np.mean(valid_aps))\n",
    "\n",
    "        mean_aps = np.mean(valid_aps)\n",
    "        \n",
    "        if mean_aps > best_score: \n",
    "            model_to_save = model.module if hasattr(model, 'module') else model \n",
    "            output_model_file = os.path.join(global_params['output_dir'], global_params['best_name'])\n",
    "            create_folder(global_params['output_dir'])\n",
    "            \n",
    "            torch.save(model_to_save.state_dict(), output_model_file)\n",
    "            best_score = mean_aps\n",
    "            \n",
    "        print('valid score: {}'.format(np.mean(valid_aps)))\n",
    "        print()\n",
    "        \n",
    "        wandb.log({'f1': np.mean(train_f1), 'loss': np.mean(train_loss), 'valid_f1': valid_f1, 'valid_auc':valid_auc,\n",
    "                      'valid_aps':valid_aps, 'valid_loss':np.mean(valid_loss), 'best_aps':best_score})\n",
    "            \n",
    "\n",
    "    test_f1, test_auc, test_aps, test_recall, test_precision, test_loss, test_time, test_hidden_states, test_masks = evaluating(model_to_save, config, testload, global_params, mlb)\n",
    "        \n",
    "    wandb.log({'test_f1':test_f1, 'test_auc': test_auc, 'test_aps': test_aps, 'test_recall': test_recall, 'test_precision':test_precision})\n",
    "\n",
    "    if test_auc >= best_score:\n",
    "        wandb.alert(\n",
    "        title=\"Higher auc\", \n",
    "        text=f\"3years: You reach your higher roc: {test_auc}\",\n",
    "        )\n",
    "        torch.save(model_to_save.state_dict(), f'Tabular_BEHRT/Outs/clf_model_{test_auc}')\n",
    "\n",
    "    return model_to_save, test_f1, test_auc, test_aps, test_recall, test_precision, test_loss, train_hidden_states_per_epoch, valid_hidden_states_per_epoch, train_targets_per_epoch, train_mask_per_epoch, valid_mask_per_epoch, valid_f1_list, train_f1_list, valid_loss_list, train_loss_list,  valid_auc_list , valid_aps_list\n",
    "   \n",
    "    \n",
    "def train_model(config=None, train=train, test=test, mlb=mlb, inputs ='inputs_normal_range_and_delta_age_and_delays_preprocessed', class_weights = False, cross_val = False, n_splits=5, seed=2):\n",
    "    empty_cuda()\n",
    "    with wandb.init(config = config) as run: \n",
    "        config = wandb.config\n",
    "   \n",
    "        set_seed(seed)\n",
    "        test.index = range(len(test))\n",
    "        test.patid = range(len(test))\n",
    "        test_loader = load_data(test, config, sampling=False)\n",
    "\n",
    "        if cross_val: \n",
    "\n",
    "            tot_train_f1, tot_train_loss, tot_val_f1, tot_val_loss, tot_val_auc, tot_val_aps, tot_test_f1, tot_test_loss, tot_test_auc, tot_test_aps, tot_test_recall, tot_test_precision = [], [], [], [], [], [], [], [], [], [], [], []\n",
    "            n_splits = n_splits\n",
    "            skf = StratifiedKFold(n_splits = n_splits)\n",
    "\n",
    "\n",
    "            for i, (train_idx, valid_idx) in enumerate(skf.split(train.drop(['label'], axis=1), train['label'])):\n",
    "                train_df = train.iloc[train_idx]\n",
    "                cv_df = train.iloc[valid_idx]\n",
    "\n",
    "                train_df.index = range(train_df.shape[0])\n",
    "                train_df.patid = range(train_df.shape[0])\n",
    "                X_tr = train_df[inputs]\n",
    "                y_tr = train_df[inputs]\n",
    "\n",
    "                cv_df.index = range(cv_df.shape[0])\n",
    "                cv_df.patid = range(cv_df.shape[0])\n",
    "\n",
    "                if class_weigths:\n",
    "                    train_loader = load_data(train_df, config, sampling = False)\n",
    "                else:\n",
    "                    train_loader = load_data(train_df, config, X_tr=X_tr, y_tr=y_tr)\n",
    "\n",
    "                valid_loader = load_data(cv_df, config, sampling = False)\n",
    "\n",
    "                model, optim= define_model(config, pretraining = False)\n",
    "\n",
    "                if config['freeze_weights']:\n",
    "                   # param_to_freeze = ['bert.embeddings.word_embeddings.weight','bert.embeddings.segment_embeddings.weight','bert.embeddings.modalities_embeddings.weight','bert.embeddings.age_embeddings.weight', 'bert.embeddings.delays_embeddings.weight', 'bert.embeddings.NPI_embeddings.weight', 'bert.embeddings.posi_embeddings.weight']\n",
    "                    for name, param in model.named_parameters():\n",
    "                       # if name in param_to_freeze:\n",
    "                        param.requires_grad = False\n",
    "\n",
    "                trained_model, test_f1, test_auc, test_aps,  test_recall, test_precision, test_loss, train_hidden_states_per_epoch, valid_hidden_states_per_epoch, train_targets_per_epoch, train_mask_per_epoch, valid_mask_per_epoch,  valid_f1_list, train_f1_list, valid_loss_list, train_loss_list, valid_auc_list , valid_aps_list = train_per_epochs(model, otpim, config, train_loader, valid_loader, test_loader, mlb)\n",
    "\n",
    "\n",
    "            tot_train_f1.append(np.mean(train_f1_list))\n",
    "            tot_train_loss.append(np.mean(train_loss_list))\n",
    "            tot_val_f1.append(np.mean(valid_f1_list))\n",
    "            tot_val_auc.append(np.mean(valid_auc_list))\n",
    "            tot_val_aps.append(np.mean(valid_aps_list))\n",
    "            tot_test_f1.append(test_f1)\n",
    "            tot_test_auc.append(test_auc)\n",
    "            tot_test_aps.append(test_aps)\n",
    "            tot_test_recall.append(test_recall)\n",
    "            tot_test_loss.appen(test_loss)\n",
    "\n",
    "            return trained_model, tot_train_f1, tot_train_loss, tot_val_f1, tot_val_loss, tot_val_auc, tot_val_aps, tot_test_f1, tot_test_loss, tot_test_auc, tot_test_aps, tot_test_recall, tot_test_precision, train_hidden_states_per_epoch, valid_hidden_states_per_epoch, train_targets_per_epoch, train_mask_per_epoch, valid_mask_per_epoch, \n",
    "\n",
    "        else: \n",
    "\n",
    "\n",
    "            X_train, X_valid, y_train, y_valid = train_test_split(train.drop(['label'], axis=1), train['label'], test_size=0.2, random_state=42, stratify=train['label'])\n",
    "\n",
    "            train_df = pd.concat([X_train, y_train], axis=1)\n",
    "            cv_df = pd.concat([X_valid, y_valid], axis=1)\n",
    "\n",
    "            train_df.index = range(train_df.shape[0])\n",
    "            train_df.patid = range(train_df.shape[0])\n",
    "\n",
    "            X_tr = train_df[inputs]\n",
    "            y_tr = train_df['label']\n",
    "\n",
    "            cv_df.index = range(cv_df.shape[0])\n",
    "            cv_df.patid = range(cv_df.shape[0])\n",
    "\n",
    "            if class_weights:\n",
    "                train_loader = load_data(train_df, config, sampling = False)\n",
    "            else:\n",
    "                train_loader = load_data(train_df, config, X_tr=X_tr, y_tr=y_tr)\n",
    "\n",
    "            valid_loader = load_data(cv_df, config, sampling=False)\n",
    "            model, optim = define_model(config, pretraining=False)\n",
    "\n",
    "            if config['freeze_weights']:\n",
    "                for param in model.bert.parameters():\n",
    "                    param.requires_grad = False\n",
    "\n",
    "            trained_model, test_f1, test_auc, test_aps, test_recall, test_precision, test_loss, train_hidden_states_per_epoch, valid_hidden_states_per_epoch, train_targets_per_epoch, train_mask_per_epoch, valid_mask_per_epoch,  valid_f1_list, train_f1_list, valid_loss_list, train_loss_list, valid_auc_list, valid_aps_list = train_per_epochs(model, optim, config, train_loader, valid_loader, test_loader, mlb)\n",
    "        \n",
    "        return trained_model, np.mean(train_f1_list), np.mean(train_loss_list), np.mean(valid_f1_list), np.mean(valid_loss_list), np.mean(valid_auc_list), np.mean(valid_aps_list), test_f1, test_loss, test_auc, test_aps, test_recall, test_precision, train_hidden_states_per_epoch, valid_hidden_states_per_epoch, train_targets_per_epoch, train_mask_per_epoch, valid_mask_per_epoch, \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95786cd-e34a-459f-85c3-c3a9a864fb0b",
   "metadata": {},
   "source": [
    "### 5. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21137ade-056a-4f93-a31b-b6ee669097f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "       \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    start = time.time()\n",
    "\n",
    "    ## lauch W&B tuning with 15 runs\n",
    "    \n",
    "    wandb.agent(sweep_id = sweep_id, function=train_model, count=15)\n",
    "  \n",
    "    trained_model, train_f1, train_loss, valid_f1, valid_loss, valid_auc , valid_aps, test_f1, test_loss, test_auc, test_aps, test_recall, test_precision, train_hidden_states_per_epoch, valid_hidden_states_per_epoch, train_targets_per_epoch, train_mask_per_epoch, valid_mask_per_epoch = train_model(config = config)\n",
    "    print(\"\")\n",
    "    print(\"Training complete!\")\n",
    "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(end-start)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
